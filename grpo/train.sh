#!/bin/bash
# ============================================
# GRPO (Group Relative Policy Optimization) è®­ç»ƒè„šæœ¬
# ============================================
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    âš™ï¸  é…ç½®åŒºåŸŸ (è¯·åœ¨æ­¤ä¿®æ”¹)                      â•‘
# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
# â•‘  ä¿®æ”¹ä¸‹é¢çš„å‚æ•°æ¥é…ç½®è®­ç»ƒï¼Œä¸éœ€è¦åœ¨å‘½ä»¤è¡Œè¾“å…¥                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ------------------------------
# ğŸ“ æ¨¡å‹é…ç½®
# ------------------------------
MODEL_PATH="Qwen/Qwen2.5-0.5B"              # æ¨¡å‹è·¯å¾„ (HuggingFaceæˆ–æœ¬åœ°è·¯å¾„)

# ------------------------------
# ğŸ“‚ æ•°æ®é…ç½®
# ------------------------------
TRAIN_DATA="${SCRIPT_DIR}/data/example_grpo.parquet"  # è®­ç»ƒæ•°æ®è·¯å¾„ (.jsonæˆ–.parquet)
VAL_DATA=""                                            # éªŒè¯æ•°æ®è·¯å¾„ (å¯é€‰)
MAX_PROMPT_LENGTH=512                                  # æœ€å¤§æç¤ºè¯é•¿åº¦
MAX_RESPONSE_LENGTH=1024                               # æœ€å¤§å›å¤é•¿åº¦

# ------------------------------
# ğŸ¯ è®­ç»ƒé…ç½®
# ------------------------------
TRAIN_BATCH_SIZE=256                  # è®­ç»ƒæ‰¹å¤§å°
MINI_BATCH_SIZE=64                    # mini batchå¤§å°
MICRO_BATCH_SIZE=16                   # micro batchå¤§å° (æ ¹æ®æ˜¾å­˜è°ƒæ•´)
LEARNING_RATE="1e-6"                  # å­¦ä¹ ç‡
TOTAL_EPOCHS=15                       # æ€»è®­ç»ƒè½®æ•°

# ------------------------------
# ğŸ”§ GRPOç®—æ³•é…ç½® (æ ¸å¿ƒå‚æ•°)
# ------------------------------
ROLLOUT_N=5                           # æ¯ä¸ªprompté‡‡æ ·æ•° (GRPOæ ¸å¿ƒå‚æ•°ï¼Œå»ºè®®5-8)
ROLLOUT_TEMPERATURE=1.0               # é‡‡æ ·æ¸©åº¦
NORM_ADV_BY_STD=true                  # æŒ‰æ ‡å‡†å·®å½’ä¸€åŒ–ä¼˜åŠ¿

# ------------------------------
# ğŸ“Š KLé…ç½®
# ------------------------------
USE_KL_LOSS=true                      # æ˜¯å¦ä½¿ç”¨KLæŸå¤±
KL_LOSS_COEF=0.001                    # KLæŸå¤±ç³»æ•°
KL_LOSS_TYPE="low_var_kl"             # KLæŸå¤±ç±»å‹

# ------------------------------
# ğŸ† Reward Functioné…ç½®
# ------------------------------
# è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°æ–‡ä»¶è·¯å¾„ (ç•™ç©ºä½¿ç”¨é»˜è®¤)
REWARD_FUNC_PATH="${SCRIPT_DIR}/reward_func.py"
REWARD_FUNC_NAME="compute_score"      # å¥–åŠ±å‡½æ•°åç§°

# ------------------------------
# ğŸš€ æ¨ç†é…ç½® (vLLM)
# ------------------------------
ROLLOUT_TP_SIZE=2                     # æ¨ç†å¼ é‡å¹¶è¡Œå¤§å°
GPU_MEMORY_UTILIZATION=0.6            # vLLMæ˜¾å­˜åˆ©ç”¨ç‡

# ------------------------------
# ğŸ’» åˆ†å¸ƒå¼é…ç½®
# ------------------------------
N_GPUS=8                              # GPUæ•°é‡
NNODES=1                              # èŠ‚ç‚¹æ•°é‡

# ------------------------------
# ğŸ’¾ è¾“å‡ºé…ç½®
# ------------------------------
OUTPUT_DIR="${SCRIPT_DIR}/outputs"                      # è¾“å‡ºç›®å½•
EXPERIMENT_NAME="grpo_$(date +%Y%m%d_%H%M%S)"          # å®éªŒåç§°
SAVE_FREQ=20                                            # ä¿å­˜é¢‘ç‡ (æ¯Næ­¥)
TEST_FREQ=5                                             # æµ‹è¯•é¢‘ç‡

# ------------------------------
# ğŸ“Š WandBç›‘æ§é…ç½®
# ------------------------------
USE_WANDB=true                        # æ˜¯å¦å¯ç”¨WandBç›‘æ§ (true/false)
WANDB_PROJECT="verl_grpo"             # WandBé¡¹ç›®åç§°
WANDB_ENTITY=""                       # WandBå›¢é˜Ÿ/ç”¨æˆ·å (ç•™ç©ºä½¿ç”¨é»˜è®¤)
WANDB_RUN_NAME=""                     # WandBè¿è¡Œåç§° (ç•™ç©ºä½¿ç”¨EXPERIMENT_NAME)
# æ³¨æ„: éœ€è¦å…ˆè¿è¡Œ wandb login æˆ–è®¾ç½® WANDB_API_KEY ç¯å¢ƒå˜é‡

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    é…ç½®åŒºåŸŸç»“æŸ                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# ===========================================
# ä»¥ä¸‹æ˜¯è„šæœ¬é€»è¾‘ï¼Œä¸€èˆ¬ä¸éœ€è¦ä¿®æ”¹
# ===========================================

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}   verl GRPO Training Script${NC}"
echo -e "${GREEN}   (No Critic Required)${NC}"
echo -e "${GREEN}========================================${NC}"

# æ•°æ®å‡†å¤‡
echo -e "${YELLOW}[1/4] å‡†å¤‡æ•°æ®...${NC}"

if [[ "$TRAIN_DATA" == *.json ]]; then
    echo "è½¬æ¢JSONä¸ºParquet..."
    PARQUET_PATH="${TRAIN_DATA%.json}.parquet"
    python3 -c "
import sys
sys.path.insert(0, '${PROJECT_ROOT}')
from common.data_utils import json_to_parquet
json_to_parquet('${TRAIN_DATA}', '${PARQUET_PATH}')
"
    TRAIN_DATA="$PARQUET_PATH"
fi

if [[ ! -f "$TRAIN_DATA" ]]; then
    echo -e "${RED}é”™è¯¯: è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: $TRAIN_DATA${NC}"
    exit 1
fi

# æ£€æŸ¥Reward Function
echo -e "${YELLOW}[2/4] æ£€æŸ¥Reward Function...${NC}"

if [[ -f "$REWARD_FUNC_PATH" ]]; then
    echo -e "${GREEN}ä½¿ç”¨è‡ªå®šä¹‰Reward Function: $REWARD_FUNC_PATH${NC}"
else
    echo -e "${BLUE}ä½¿ç”¨é»˜è®¤Reward Function${NC}"
    REWARD_FUNC_PATH=""
fi

# ç¯å¢ƒæ£€æŸ¥
echo -e "${YELLOW}[3/4] æ£€æŸ¥ç¯å¢ƒ...${NC}"

if command -v nvidia-smi &> /dev/null; then
    GPU_COUNT=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
    echo -e "${GREEN}æ£€æµ‹åˆ° $GPU_COUNT ä¸ªGPU${NC}"
else
    echo -e "${RED}é”™è¯¯: GRPOè®­ç»ƒéœ€è¦GPU${NC}"
    exit 1
fi

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$OUTPUT_DIR"/{logs,checkpoints}

# æ‰“å°é…ç½®æ‘˜è¦
echo -e "${YELLOW}[4/4] å¯åŠ¨GRPOè®­ç»ƒ...${NC}"
echo ""
echo "============================================"
echo "GRPOè®­ç»ƒé…ç½®æ‘˜è¦:"
echo "============================================"
echo "æ¨¡å‹è·¯å¾„:       $MODEL_PATH"
echo "è®­ç»ƒæ•°æ®:       $TRAIN_DATA"
echo "æ‰¹å¤§å°:         $TRAIN_BATCH_SIZE"
echo "å­¦ä¹ ç‡:         $LEARNING_RATE"
echo "è®­ç»ƒè½®æ•°:       $TOTAL_EPOCHS"
echo "æ¯prompté‡‡æ ·:   $ROLLOUT_N (GRPOå…³é”®å‚æ•°)"
echo "KLç³»æ•°:         $KL_LOSS_COEF"
echo "GPUæ•°é‡:        $N_GPUS"
echo "è¾“å‡ºç›®å½•:       $OUTPUT_DIR"
echo "WandBç›‘æ§:      $USE_WANDB"
if [[ "$USE_WANDB" == "true" ]]; then
    echo "WandBé¡¹ç›®:      $WANDB_PROJECT"
fi
echo "============================================"
echo ""

# é…ç½®WandB
LOGGER_CONFIG='["console"]'
if [[ "$USE_WANDB" == "true" ]]; then
    LOGGER_CONFIG='["console","wandb"]'
    export WANDB_PROJECT="$WANDB_PROJECT"
    if [[ -n "$WANDB_ENTITY" ]]; then
        export WANDB_ENTITY="$WANDB_ENTITY"
    fi
    if [[ -n "$WANDB_RUN_NAME" ]]; then
        export WANDB_RUN_NAME="$WANDB_RUN_NAME"
    else
        export WANDB_RUN_NAME="$EXPERIMENT_NAME"
    fi
fi

# æ„å»ºReward Functionå‚æ•°
REWARD_ARGS=""
if [[ -n "$REWARD_FUNC_PATH" ]]; then
    REWARD_ARGS="
    custom_reward_function.path=$REWARD_FUNC_PATH
    custom_reward_function.name=$REWARD_FUNC_NAME
    "
fi

# å¯åŠ¨è®­ç»ƒ
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    algorithm.norm_adv_by_std_in_grpo=$NORM_ADV_BY_STD \
    \
    data.train_files="$TRAIN_DATA" \
    data.val_files="$VAL_DATA" \
    data.train_batch_size=$TRAIN_BATCH_SIZE \
    data.max_prompt_length=$MAX_PROMPT_LENGTH \
    data.max_response_length=$MAX_RESPONSE_LENGTH \
    \
    actor_rollout_ref.model.path="$MODEL_PATH" \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.model.use_remove_padding=True \
    \
    actor_rollout_ref.actor.strategy=fsdp \
    actor_rollout_ref.actor.optim.lr=$LEARNING_RATE \
    actor_rollout_ref.actor.ppo_mini_batch_size=$MINI_BATCH_SIZE \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=$MICRO_BATCH_SIZE \
    actor_rollout_ref.actor.ppo_epochs=1 \
    actor_rollout_ref.actor.grad_clip=1.0 \
    actor_rollout_ref.actor.use_kl_loss=$USE_KL_LOSS \
    actor_rollout_ref.actor.kl_loss_coef=$KL_LOSS_COEF \
    actor_rollout_ref.actor.kl_loss_type=$KL_LOSS_TYPE \
    \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.n=$ROLLOUT_N \
    actor_rollout_ref.rollout.temperature=$ROLLOUT_TEMPERATURE \
    actor_rollout_ref.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE \
    actor_rollout_ref.rollout.gpu_memory_utilization=$GPU_MEMORY_UTILIZATION \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=160 \
    \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=160 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    \
    trainer.n_gpus_per_node=$N_GPUS \
    trainer.nnodes=$NNODES \
    trainer.total_epochs=$TOTAL_EPOCHS \
    trainer.save_freq=$SAVE_FREQ \
    trainer.test_freq=$TEST_FREQ \
    trainer.project_name=verl_grpo \
    trainer.experiment_name="$EXPERIMENT_NAME" \
    trainer.default_local_dir="$OUTPUT_DIR/checkpoints" \
    trainer.logger="$LOGGER_CONFIG" \
    $REWARD_ARGS

echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}   GRPOè®­ç»ƒå®Œæˆ!${NC}"
echo -e "${GREEN}========================================${NC}"
echo "æ¨¡å‹ä¿å­˜åœ¨: $OUTPUT_DIR/checkpoints"
