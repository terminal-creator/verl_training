#!/bin/bash
# ============================================
# PPO (Proximal Policy Optimization) ËÆ≠ÁªÉËÑöÊú¨
# ============================================
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
# ‚ïë                    ‚öôÔ∏è  ÈÖçÁΩÆÂå∫Âüü (ËØ∑Âú®Ê≠§‰øÆÊîπ)                      ‚ïë
# ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
# ‚ïë  ‰øÆÊîπ‰∏ãÈù¢ÁöÑÂèÇÊï∞Êù•ÈÖçÁΩÆËÆ≠ÁªÉÔºå‰∏çÈúÄË¶ÅÂú®ÂëΩ‰ª§Ë°åËæìÂÖ•                         ‚ïë
# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

# ------------------------------
# üìÅ Ê®°ÂûãÈÖçÁΩÆ
# ------------------------------
MODEL_PATH="Qwen/Qwen2.5-0.5B"              # ActorÊ®°ÂûãË∑ØÂæÑ (HuggingFaceÊàñÊú¨Âú∞Ë∑ØÂæÑ)
REWARD_MODEL_PATH=""                         # Reward ModelË∑ØÂæÑ (ÂèØÈÄâÔºåÁïôÁ©∫Âàô‰∏ç‰ΩøÁî®)

# ------------------------------
# üìÇ Êï∞ÊçÆÈÖçÁΩÆ
# ------------------------------
TRAIN_DATA="${SCRIPT_DIR}/data/example_ppo.parquet"   # ËÆ≠ÁªÉÊï∞ÊçÆË∑ØÂæÑ (.jsonÊàñ.parquet)
VAL_DATA=""                                            # È™åËØÅÊï∞ÊçÆË∑ØÂæÑ (ÂèØÈÄâ)
MAX_PROMPT_LENGTH=512                                  # ÊúÄÂ§ßÊèêÁ§∫ËØçÈïøÂ∫¶
MAX_RESPONSE_LENGTH=512                                # ÊúÄÂ§ßÂõûÂ§çÈïøÂ∫¶

# ------------------------------
# üéØ ËÆ≠ÁªÉÈÖçÁΩÆ
# ------------------------------
TRAIN_BATCH_SIZE=256                  # ËÆ≠ÁªÉÊâπÂ§ßÂ∞è
PPO_MINI_BATCH_SIZE=64                # PPO mini batchÂ§ßÂ∞è
PPO_MICRO_BATCH_SIZE=8                # PPO micro batchÂ§ßÂ∞è (Ê†πÊçÆÊòæÂ≠òË∞ÉÊï¥)
LEARNING_RATE="1e-6"                  # ActorÂ≠¶‰π†Áéá
TOTAL_EPOCHS=15                       # ÊÄªËÆ≠ÁªÉËΩÆÊï∞

# ------------------------------
# üîß PPOÁÆóÊ≥ïÈÖçÁΩÆ
# ------------------------------
CLIP_RATIO=0.2                        # PPOË£ÅÂâ™ËåÉÂõ¥
GAE_GAMMA=1.0                         # GAEÊäòÊâ£Âõ†Â≠ê
GAE_LAMBDA=0.95                       # GAE lambda
KL_COEF=0.001                         # KLÊÉ©ÁΩöÁ≥ªÊï∞
PPO_EPOCHS=1                          # ÊØèÊâπÊï∞ÊçÆÁöÑPPOÊõ¥Êñ∞ËΩÆÊï∞

# ------------------------------
# üìä CriticÈÖçÁΩÆ
# ------------------------------
CRITIC_LR="1e-5"                      # CriticÂ≠¶‰π†Áéá
CRITIC_EPOCHS=1                       # CriticÊõ¥Êñ∞ËΩÆÊï∞
CLIPRANGE_VALUE=0.5                   # ÂÄºÂáΩÊï∞Ë£ÅÂâ™ËåÉÂõ¥

# ------------------------------
# üöÄ Êé®ÁêÜÈÖçÁΩÆ (vLLM)
# ------------------------------
ROLLOUT_N=1                           # ÊØè‰∏™promptÈááÊ†∑Êï∞
ROLLOUT_TEMPERATURE=1.0               # ÈááÊ†∑Ê∏©Â∫¶
ROLLOUT_TP_SIZE=1                     # Êé®ÁêÜÂº†ÈáèÂπ∂Ë°åÂ§ßÂ∞è
GPU_MEMORY_UTILIZATION=0.5            # vLLMÊòæÂ≠òÂà©Áî®Áéá

# ------------------------------
# üíª ÂàÜÂ∏ÉÂºèÈÖçÁΩÆ
# ------------------------------
N_GPUS=8                              # GPUÊï∞Èáè
NNODES=1                              # ËäÇÁÇπÊï∞Èáè

# ------------------------------
# üíæ ËæìÂá∫ÈÖçÁΩÆ
# ------------------------------
OUTPUT_DIR="${SCRIPT_DIR}/outputs"                      # ËæìÂá∫ÁõÆÂΩï
EXPERIMENT_NAME="ppo_$(date +%Y%m%d_%H%M%S)"           # ÂÆûÈ™åÂêçÁß∞
SAVE_FREQ=20                                            # ‰øùÂ≠òÈ¢ëÁéá (ÊØèNÊ≠•)
TEST_FREQ=5                                             # ÊµãËØïÈ¢ëÁéá

# ------------------------------
# üìä WandBÁõëÊéßÈÖçÁΩÆ
# ------------------------------
USE_WANDB=true                        # ÊòØÂê¶ÂêØÁî®WandBÁõëÊéß (true/false)
WANDB_PROJECT="verl_ppo"              # WandBÈ°πÁõÆÂêçÁß∞
WANDB_ENTITY=""                       # WandBÂõ¢Èòü/Áî®Êà∑Âêç (ÁïôÁ©∫‰ΩøÁî®ÈªòËÆ§)
WANDB_RUN_NAME=""                     # WandBËøêË°åÂêçÁß∞ (ÁïôÁ©∫‰ΩøÁî®EXPERIMENT_NAME)
# Ê≥®ÊÑè: ÈúÄË¶ÅÂÖàËøêË°å wandb login ÊàñËÆæÁΩÆ WANDB_API_KEY ÁéØÂ¢ÉÂèòÈáè

# ------------------------------
# ü§ñ API Reward ModelÈÖçÁΩÆ (LLM-as-a-Judge)
# ------------------------------
# ÊòØÂê¶‰ΩøÁî®API‰Ωú‰∏∫Reward Model
USE_API_REWARD=false

# APIÁ±ªÂûã: dashscope / openai / gemini / claude
RM_API_TYPE="dashscope"

# APIÊ®°ÂûãÂêçÁß∞
# DashScope: qwen-plus, qwen-max, qwen-turbo
# OpenAI: gpt-4o-mini, gpt-4o
# Gemini: gemini-1.5-flash, gemini-1.5-pro
# Claude: claude-3-haiku-20240307, claude-3-sonnet-20240229
RM_MODEL="qwen-plus"

# È¢ÑËÆæËØÑÂàÜÊ®°Êùø: math / code / dialogue / safety (ÁïôÁ©∫‰ΩøÁî®ÈªòËÆ§)
RM_PROMPT_PRESET=""

# Ëá™ÂÆö‰πâÁ≥ªÁªüÊèêÁ§∫ËØç (ÂèØÈÄâÔºåÁïôÁ©∫‰ΩøÁî®ÈªòËÆ§)
RM_SYSTEM_PROMPT=""

# Ëá™ÂÆö‰πâËØÑÂàÜÊèêÁ§∫ËØç (ÂèØÈÄâÔºåÁïôÁ©∫‰ΩøÁî®ÈªòËÆ§)
# ÂèØÁî®ÂèòÈáè: {prompt}, {response}, {ground_truth}
RM_SCORING_PROMPT=""

# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
# ‚ïë                    ÈÖçÁΩÆÂå∫ÂüüÁªìÊùü                                   ‚ïë
# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù


# ===========================================
# ‰ª•‰∏ãÊòØËÑöÊú¨ÈÄªËæëÔºå‰∏ÄËà¨‰∏çÈúÄË¶Å‰øÆÊîπ
# ===========================================

# È¢úËâ≤ËæìÂá∫
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}   verl PPO Training Script${NC}"
echo -e "${GREEN}========================================${NC}"

# Êï∞ÊçÆÂáÜÂ§á
echo -e "${YELLOW}[1/4] ÂáÜÂ§áÊï∞ÊçÆ...${NC}"

if [[ "$TRAIN_DATA" == *.json ]]; then
    echo "ËΩ¨Êç¢JSON‰∏∫Parquet..."
    PARQUET_PATH="${TRAIN_DATA%.json}.parquet"
    python3 -c "
import sys
sys.path.insert(0, '${PROJECT_ROOT}')
from common.data_utils import json_to_parquet
json_to_parquet('${TRAIN_DATA}', '${PARQUET_PATH}')
"
    TRAIN_DATA="$PARQUET_PATH"
fi

if [[ ! -f "$TRAIN_DATA" ]]; then
    echo -e "${RED}ÈîôËØØ: ËÆ≠ÁªÉÊï∞ÊçÆ‰∏çÂ≠òÂú®: $TRAIN_DATA${NC}"
    exit 1
fi

# ÁéØÂ¢ÉÊ£ÄÊü•
echo -e "${YELLOW}[2/4] Ê£ÄÊü•ÁéØÂ¢É...${NC}"

if command -v nvidia-smi &> /dev/null; then
    GPU_COUNT=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
    echo -e "${GREEN}Ê£ÄÊµãÂà∞ $GPU_COUNT ‰∏™GPU${NC}"
else
    echo -e "${RED}ÈîôËØØ: PPOËÆ≠ÁªÉÈúÄË¶ÅGPU${NC}"
    exit 1
fi

# ÂàõÂª∫ËæìÂá∫ÁõÆÂΩï
echo -e "${YELLOW}[3/4] ÂáÜÂ§áËæìÂá∫ÁõÆÂΩï...${NC}"
mkdir -p "$OUTPUT_DIR"/{logs,checkpoints}

# ÂêØÂä®ËÆ≠ÁªÉ
echo -e "${YELLOW}[4/4] ÂêØÂä®PPOËÆ≠ÁªÉ...${NC}"

# Reward ModelÈÖçÁΩÆ
RM_ARGS=""
REWARD_FUNC_PATH=""

if [[ "$USE_API_REWARD" == "true" ]]; then
    echo -e "${BLUE}‰ΩøÁî®API Reward Model (LLM-as-a-Judge)${NC}"
    echo -e "${BLUE}  APIÁ±ªÂûã: $RM_API_TYPE${NC}"
    echo -e "${BLUE}  Ê®°Âûã: $RM_MODEL${NC}"

    export RM_API_TYPE="$RM_API_TYPE"
    export RM_MODEL="$RM_MODEL"

    if [[ -n "$RM_PROMPT_PRESET" ]]; then
        echo -e "${BLUE}  È¢ÑËÆæÊ®°Êùø: $RM_PROMPT_PRESET${NC}"
        export RM_PROMPT_PRESET="$RM_PROMPT_PRESET"
    fi

    if [[ -n "$RM_SYSTEM_PROMPT" ]]; then
        export RM_SYSTEM_PROMPT="$RM_SYSTEM_PROMPT"
    fi
    if [[ -n "$RM_SCORING_PROMPT" ]]; then
        export RM_SCORING_PROMPT="$RM_SCORING_PROMPT"
    fi

    REWARD_FUNC_PATH="${SCRIPT_DIR}/api_reward.py"
    RM_ARGS="
    custom_reward_function.path=$REWARD_FUNC_PATH
    custom_reward_function.name=compute_score
    "

elif [[ -n "$REWARD_MODEL_PATH" ]]; then
    echo -e "${BLUE}‰ΩøÁî®Reward Model: $REWARD_MODEL_PATH${NC}"
    RM_ARGS="
    reward_model.enable=True
    reward_model.model.path=$REWARD_MODEL_PATH
    reward_model.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE
    reward_model.rollout.gpu_memory_utilization=$GPU_MEMORY_UTILIZATION
    "
else
    echo -e "${BLUE}‰ΩøÁî®ÈªòËÆ§Reward Function${NC}"
fi

# ÊâìÂç∞ÈÖçÁΩÆÊëòË¶Å
echo ""
echo "============================================"
echo "PPOËÆ≠ÁªÉÈÖçÁΩÆÊëòË¶Å:"
echo "============================================"
echo "ActorÊ®°Âûã:      $MODEL_PATH"
if [[ "$USE_API_REWARD" == "true" ]]; then
    echo "Reward:         API ($RM_API_TYPE / $RM_MODEL)"
elif [[ -n "$REWARD_MODEL_PATH" ]]; then
    echo "Reward Model:   $REWARD_MODEL_PATH"
else
    echo "Reward:         ÈªòËÆ§reward function"
fi
echo "ËÆ≠ÁªÉÊï∞ÊçÆ:       $TRAIN_DATA"
echo "ÊâπÂ§ßÂ∞è:         $TRAIN_BATCH_SIZE"
echo "Â≠¶‰π†Áéá:         $LEARNING_RATE"
echo "ËÆ≠ÁªÉËΩÆÊï∞:       $TOTAL_EPOCHS"
echo "KLÁ≥ªÊï∞:         $KL_COEF"
echo "PPOË£ÅÂâ™:        $CLIP_RATIO"
echo "GPUÊï∞Èáè:        $N_GPUS"
echo "ËæìÂá∫ÁõÆÂΩï:       $OUTPUT_DIR"
echo "WandBÁõëÊéß:      $USE_WANDB"
if [[ "$USE_WANDB" == "true" ]]; then
    echo "WandBÈ°πÁõÆ:      $WANDB_PROJECT"
fi
echo "============================================"
echo ""

# ÈÖçÁΩÆWandB
LOGGER_CONFIG='["console"]'
if [[ "$USE_WANDB" == "true" ]]; then
    LOGGER_CONFIG='["console","wandb"]'
    export WANDB_PROJECT="$WANDB_PROJECT"
    if [[ -n "$WANDB_ENTITY" ]]; then
        export WANDB_ENTITY="$WANDB_ENTITY"
    fi
    if [[ -n "$WANDB_RUN_NAME" ]]; then
        export WANDB_RUN_NAME="$WANDB_RUN_NAME"
    else
        export WANDB_RUN_NAME="$EXPERIMENT_NAME"
    fi
fi

# ÂêØÂä®ËÆ≠ÁªÉ
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=gae \
    algorithm.gamma=$GAE_GAMMA \
    algorithm.lam=$GAE_LAMBDA \
    algorithm.kl_ctrl.type=fixed \
    algorithm.kl_ctrl.kl_coef=$KL_COEF \
    \
    data.train_files="$TRAIN_DATA" \
    data.val_files="$VAL_DATA" \
    data.train_batch_size=$TRAIN_BATCH_SIZE \
    data.max_prompt_length=$MAX_PROMPT_LENGTH \
    data.max_response_length=$MAX_RESPONSE_LENGTH \
    \
    actor_rollout_ref.model.path="$MODEL_PATH" \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.model.use_remove_padding=True \
    \
    actor_rollout_ref.actor.strategy=fsdp \
    actor_rollout_ref.actor.optim.lr=$LEARNING_RATE \
    actor_rollout_ref.actor.ppo_mini_batch_size=$PPO_MINI_BATCH_SIZE \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=$PPO_MICRO_BATCH_SIZE \
    actor_rollout_ref.actor.ppo_epochs=$PPO_EPOCHS \
    actor_rollout_ref.actor.clip_ratio=$CLIP_RATIO \
    actor_rollout_ref.actor.grad_clip=1.0 \
    \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.n=$ROLLOUT_N \
    actor_rollout_ref.rollout.temperature=$ROLLOUT_TEMPERATURE \
    actor_rollout_ref.rollout.tensor_model_parallel_size=$ROLLOUT_TP_SIZE \
    actor_rollout_ref.rollout.gpu_memory_utilization=$GPU_MEMORY_UTILIZATION \
    \
    actor_rollout_ref.ref.fsdp_config.param_offload=False \
    \
    critic.enable=True \
    critic.optim.lr=$CRITIC_LR \
    critic.ppo_epochs=$CRITIC_EPOCHS \
    critic.cliprange_value=$CLIPRANGE_VALUE \
    \
    trainer.n_gpus_per_node=$N_GPUS \
    trainer.nnodes=$NNODES \
    trainer.total_epochs=$TOTAL_EPOCHS \
    trainer.save_freq=$SAVE_FREQ \
    trainer.test_freq=$TEST_FREQ \
    trainer.project_name=verl_ppo \
    trainer.experiment_name="$EXPERIMENT_NAME" \
    trainer.default_local_dir="$OUTPUT_DIR/checkpoints" \
    trainer.logger="$LOGGER_CONFIG" \
    $RM_ARGS

echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}   PPOËÆ≠ÁªÉÂÆåÊàê!${NC}"
echo -e "${GREEN}========================================${NC}"
echo "Ê®°Âûã‰øùÂ≠òÂú®: $OUTPUT_DIR/checkpoints"
