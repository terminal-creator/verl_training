#!/bin/bash
# ============================================
# SFT (Supervised Fine-Tuning) è®­ç»ƒè„šæœ¬
# ============================================
set -e

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    âš™ï¸  é…ç½®åŒºåŸŸ (è¯·åœ¨æ­¤ä¿®æ”¹)                      â•‘
# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
# â•‘  ä¿®æ”¹ä¸‹é¢çš„å‚æ•°æ¥é…ç½®è®­ç»ƒï¼Œä¸éœ€è¦åœ¨å‘½ä»¤è¡Œè¾“å…¥                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ------------------------------
# ğŸ“ æ¨¡å‹é…ç½®
# ------------------------------
MODEL_PATH="Qwen/Qwen2.5-0.5B"              # æ¨¡å‹è·¯å¾„ (HuggingFaceæˆ–æœ¬åœ°è·¯å¾„)

# ------------------------------
# ğŸ“‚ æ•°æ®é…ç½®
# ------------------------------
TRAIN_DATA="${SCRIPT_DIR}/data/example_sft.parquet"   # è®­ç»ƒæ•°æ®è·¯å¾„ (.jsonæˆ–.parquet)
VAL_DATA=""                                            # éªŒè¯æ•°æ®è·¯å¾„ (å¯é€‰)
MAX_LENGTH=2048                                        # æœ€å¤§åºåˆ—é•¿åº¦

# ------------------------------
# ğŸ¯ è®­ç»ƒé…ç½®
# ------------------------------
BATCH_SIZE=4                          # æ¯GPUæ‰¹å¤§å°
MICRO_BATCH_SIZE=1                    # micro batchå¤§å°
LEARNING_RATE="2e-5"                  # å­¦ä¹ ç‡
NUM_EPOCHS=3                          # è®­ç»ƒè½®æ•°
WARMUP_RATIO=0.1                      # é¢„çƒ­æ¯”ä¾‹
WEIGHT_DECAY=0.01                     # æƒé‡è¡°å‡
GRAD_CLIP=1.0                         # æ¢¯åº¦è£å‰ª

# ------------------------------
# ğŸ”§ LoRAé…ç½® (å¯é€‰)
# ------------------------------
USE_LORA=false                        # æ˜¯å¦ä½¿ç”¨LoRA (true/false)
LORA_RANK=8                           # LoRAç§©
LORA_ALPHA=16                         # LoRA alpha

# ------------------------------
# ğŸ’» åˆ†å¸ƒå¼é…ç½®
# ------------------------------
N_GPUS=1                              # GPUæ•°é‡
STRATEGY="fsdp"                       # è®­ç»ƒç­–ç•¥: fsdp / ddp

# ------------------------------
# ğŸ’¾ è¾“å‡ºé…ç½®
# ------------------------------
OUTPUT_DIR="${SCRIPT_DIR}/outputs"                      # è¾“å‡ºç›®å½•
EXPERIMENT_NAME="sft_$(date +%Y%m%d_%H%M%S)"           # å®éªŒåç§°
SAVE_STEPS=500                                          # ä¿å­˜é¢‘ç‡ (æ¯Næ­¥)
LOGGING_STEPS=10                                        # æ—¥å¿—é¢‘ç‡

# ------------------------------
# ğŸ“Š WandBç›‘æ§é…ç½®
# ------------------------------
USE_WANDB=true                        # æ˜¯å¦å¯ç”¨WandBç›‘æ§ (true/false)
WANDB_PROJECT="verl_sft"              # WandBé¡¹ç›®åç§°
WANDB_ENTITY=""                       # WandBå›¢é˜Ÿ/ç”¨æˆ·å (ç•™ç©ºä½¿ç”¨é»˜è®¤)
WANDB_RUN_NAME=""                     # WandBè¿è¡Œåç§° (ç•™ç©ºä½¿ç”¨EXPERIMENT_NAME)
# æ³¨æ„: éœ€è¦å…ˆè¿è¡Œ wandb login æˆ–è®¾ç½® WANDB_API_KEY ç¯å¢ƒå˜é‡

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    é…ç½®åŒºåŸŸç»“æŸ                                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


# ===========================================
# ä»¥ä¸‹æ˜¯è„šæœ¬é€»è¾‘ï¼Œä¸€èˆ¬ä¸éœ€è¦ä¿®æ”¹
# ===========================================

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}   verl SFT Training Script${NC}"
echo -e "${GREEN}========================================${NC}"

# æ•°æ®å‡†å¤‡
echo -e "${YELLOW}[1/4] æ£€æŸ¥æ•°æ®æ–‡ä»¶...${NC}"

if [[ "$TRAIN_DATA" == *.json ]]; then
    echo "æ£€æµ‹åˆ°JSONæ ¼å¼ï¼Œè½¬æ¢ä¸ºParquet..."
    PARQUET_PATH="${TRAIN_DATA%.json}.parquet"
    python3 -c "
import sys
sys.path.insert(0, '${PROJECT_ROOT}')
from common.data_utils import json_to_parquet
json_to_parquet('${TRAIN_DATA}', '${PARQUET_PATH}')
"
    TRAIN_DATA="$PARQUET_PATH"
fi

if [[ ! -f "$TRAIN_DATA" ]]; then
    echo -e "${RED}é”™è¯¯: è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $TRAIN_DATA${NC}"
    exit 1
fi

echo -e "${GREEN}è®­ç»ƒæ•°æ®: $TRAIN_DATA${NC}"

# ç¯å¢ƒæ£€æŸ¥
echo -e "${YELLOW}[2/4] æ£€æŸ¥ç¯å¢ƒ...${NC}"

if command -v nvidia-smi &> /dev/null; then
    GPU_COUNT=$(nvidia-smi --query-gpu=name --format=csv,noheader | wc -l)
    echo -e "${GREEN}æ£€æµ‹åˆ° $GPU_COUNT ä¸ªGPU${NC}"
    if [[ $N_GPUS -gt $GPU_COUNT ]]; then
        echo -e "${YELLOW}è­¦å‘Š: è¯·æ±‚ $N_GPUS ä¸ªGPUï¼Œä½†åªæœ‰ $GPU_COUNT ä¸ªå¯ç”¨${NC}"
        N_GPUS=$GPU_COUNT
    fi
else
    echo -e "${YELLOW}è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ${NC}"
    N_GPUS=0
fi

# åˆ›å»ºè¾“å‡ºç›®å½•
echo -e "${YELLOW}[3/4] å‡†å¤‡è¾“å‡ºç›®å½•...${NC}"
mkdir -p "$OUTPUT_DIR"/{logs,checkpoints}

# æ„å»ºLoRAå‚æ•°
LORA_ARGS=""
if [[ "$USE_LORA" == "true" ]]; then
    LORA_ARGS="
    actor_rollout_ref.model.lora_rank=${LORA_RANK}
    actor_rollout_ref.model.lora_alpha=${LORA_ALPHA}
    actor_rollout_ref.model.target_modules=all-linear
    "
    echo -e "${GREEN}å¯ç”¨LoRAå¾®è°ƒ: rank=${LORA_RANK}, alpha=${LORA_ALPHA}${NC}"
fi

# æ‰“å°é…ç½®æ‘˜è¦
echo -e "${YELLOW}[4/4] å¯åŠ¨è®­ç»ƒ...${NC}"
echo ""
echo "============================================"
echo "SFTè®­ç»ƒé…ç½®æ‘˜è¦:"
echo "============================================"
echo "æ¨¡å‹è·¯å¾„:     $MODEL_PATH"
echo "è®­ç»ƒæ•°æ®:     $TRAIN_DATA"
echo "æ‰¹å¤§å°:       $BATCH_SIZE"
echo "å­¦ä¹ ç‡:       $LEARNING_RATE"
echo "è®­ç»ƒè½®æ•°:     $NUM_EPOCHS"
echo "ä½¿ç”¨LoRA:     $USE_LORA"
echo "GPUæ•°é‡:      $N_GPUS"
echo "è¾“å‡ºç›®å½•:     $OUTPUT_DIR"
echo "WandBç›‘æ§:    $USE_WANDB"
if [[ "$USE_WANDB" == "true" ]]; then
    echo "WandBé¡¹ç›®:    $WANDB_PROJECT"
fi
echo "============================================"
echo ""

# é…ç½®WandB
LOGGER_CONFIG='["console"]'
if [[ "$USE_WANDB" == "true" ]]; then
    LOGGER_CONFIG='["console","wandb"]'
    export WANDB_PROJECT="$WANDB_PROJECT"
    if [[ -n "$WANDB_ENTITY" ]]; then
        export WANDB_ENTITY="$WANDB_ENTITY"
    fi
    if [[ -n "$WANDB_RUN_NAME" ]]; then
        export WANDB_RUN_NAME="$WANDB_RUN_NAME"
    else
        export WANDB_RUN_NAME="$EXPERIMENT_NAME"
    fi
fi

# å¯åŠ¨è®­ç»ƒ
python3 -m verl.trainer.main_sft \
    data.train_files="$TRAIN_DATA" \
    data.val_files="$VAL_DATA" \
    data.max_length=$MAX_LENGTH \
    \
    actor_rollout_ref.model.path="$MODEL_PATH" \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.model.use_remove_padding=True \
    \
    actor_rollout_ref.actor.strategy=$STRATEGY \
    actor_rollout_ref.actor.optim.lr=$LEARNING_RATE \
    actor_rollout_ref.actor.optim.weight_decay=$WEIGHT_DECAY \
    actor_rollout_ref.actor.optim.lr_warmup_steps_ratio=$WARMUP_RATIO \
    actor_rollout_ref.actor.grad_clip=$GRAD_CLIP \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=$MICRO_BATCH_SIZE \
    \
    trainer.total_epochs=$NUM_EPOCHS \
    trainer.n_gpus_per_node=$N_GPUS \
    trainer.save_freq=$SAVE_STEPS \
    trainer.project_name=verl_sft \
    trainer.experiment_name="$EXPERIMENT_NAME" \
    trainer.default_local_dir="$OUTPUT_DIR/checkpoints" \
    trainer.logger="$LOGGER_CONFIG" \
    $LORA_ARGS

echo ""
echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}   SFTè®­ç»ƒå®Œæˆ!${NC}"
echo -e "${GREEN}========================================${NC}"
echo "æ¨¡å‹ä¿å­˜åœ¨: $OUTPUT_DIR/checkpoints"
